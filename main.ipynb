{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a73343d-230c-4e0c-a9c6-67b0a90c2c1e",
   "metadata": {},
   "source": [
    "## Load Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "a31376e4-4eb3-42a7-a797-539798fcc00f",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from datetime import datetime\n",
    "from scipy.stats import trim_mean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d10df99d-3144-4e7d-945b-7889e98cd58e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "a949fad1-3c16-434a-bb42-3255dbc2ea65",
   "metadata": {},
   "source": [
    "file_path = 'data/Animation_Movies.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1: Data Recognition\n",
    "\n",
    "## structure of data"
   ],
   "id": "5b64e1cec5749098"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.info()\n",
    "df.describe()"
   ],
   "id": "dd29ad13d309c1b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculating Statistical Metrics for Each Numeric Column",
   "id": "154fb466930ae7ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_statistics(data_frame, Numeric_Features):\n",
    "    stats__summary = []\n",
    "\n",
    "    for feature in Numeric_Features:\n",
    "        dtype = data_frame[feature].dtype\n",
    "        min_value = data_frame[feature].min()\n",
    "        max_value = data_frame[feature].max()\n",
    "        mean_value = data_frame[feature].mean()\n",
    "        median_value = data_frame[feature].median()\n",
    "        mode_value = data_frame[feature].mode()[0]  # First mode in case there are multiple modes\n",
    "        value_range = f\"{min_value} - {max_value}\"\n",
    "\n",
    "        # Add the calculated statistics to the summary DataFrame\n",
    "        stats__summary.append({\n",
    "            'Feature': feature,\n",
    "            'Type': dtype,\n",
    "            'Min': min_value,\n",
    "            'Max': max_value,\n",
    "            'Mean': mean_value,\n",
    "            'Median': median_value,\n",
    "            'Mode': mode_value,\n",
    "            'Range': value_range\n",
    "        })\n",
    "\n",
    "    # Convert list to DataFrame\n",
    "    stats_summary_df = pd.DataFrame(stats__summary)\n",
    "\n",
    "    return stats_summary_df\n",
    "\n",
    "\n",
    "# List of numerical features for analysis\n",
    "numeric_features = ['vote_average', 'vote_count', 'runtime', 'revenue', 'budget', 'popularity']\n",
    "stats_summary = calculate_statistics(df, numeric_features)\n",
    "\n",
    "# Display the statistical summary table\n",
    "stats_summary"
   ],
   "id": "6f8f79c1de508007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detecting Outliers",
   "id": "6d987be22baf6472"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def detect_outliers(data_frame, Numeric_Features):\n",
    "    outliers_list = []\n",
    "\n",
    "    for feature in Numeric_Features:\n",
    "        Q1 = data_frame[feature].quantile(0.25)\n",
    "        Q3 = data_frame[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = data_frame[(data_frame[feature] < (Q1 - 1.5 * IQR)) | (data_frame[feature] > (Q3 + 1.5 * IQR))][\n",
    "            feature].values\n",
    "\n",
    "        # Calculate the percentage of outliers\n",
    "        total_data_points = len(data_frame[feature])\n",
    "        outlier_count = len(outliers)\n",
    "        outlier_percentage = (outlier_count / total_data_points) * 100\n",
    "\n",
    "        # Append a dictionary to the list\n",
    "        outliers_list.append({\n",
    "            'Feature': feature,\n",
    "            'Outliers': outliers,\n",
    "            'Outlier_Percentage': outlier_percentage\n",
    "        })\n",
    "\n",
    "    # Create the summary DataFrame once at the end\n",
    "    outliers__summary = pd.DataFrame(outliers_list)\n",
    "\n",
    "    return outliers__summary\n",
    "\n",
    "\n",
    "# Use the modified function\n",
    "outliers_summary = detect_outliers(df, numeric_features)\n",
    "\n",
    "# Display the outliers summary table\n",
    "outliers_summary"
   ],
   "id": "110141d63df99861",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating a Data Recognition Table",
   "id": "b92c024fd1ed404d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def summarize_data(data_frame, Numeric_Features):\n",
    "    stats__summary = calculate_statistics(data_frame, Numeric_Features)\n",
    "    outliers__summary = detect_outliers(data_frame, Numeric_Features)\n",
    "\n",
    "    # Merge statistics and outliers data\n",
    "    final__summary = stats__summary.merge(outliers__summary, on='Feature', how='left')\n",
    "    return final__summary\n",
    "\n",
    "\n",
    "# Generate the final summary\n",
    "final_summary = summarize_data(df, numeric_features)\n",
    "\n",
    "# Display the final summary\n",
    "final_summary"
   ],
   "id": "7a501d2b01c8ca89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating a Boxplot for Numerical Data",
   "id": "abb772c7be5df291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_boxplots(data_frame, columns, exclude_zero_columns=None, after_preprocessing=False, figsize=(20, 12),\n",
    "                  linewidth=1.5):\n",
    "    if exclude_zero_columns is None:\n",
    "        exclude_zero_columns = []\n",
    "\n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_plots = len(columns)\n",
    "    rows = (num_plots + 2) // 3  # Round up to the nearest integer\n",
    "    cols = 3  # Number of columns\n",
    "\n",
    "    # Set up the subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()  # Flatten to easily iterate over\n",
    "\n",
    "    # Create box-plots for each column\n",
    "    for i, column in enumerate(columns):\n",
    "        # Determine the scale of the data for scientific notation\n",
    "        max_abs_value = data_frame[column].abs().max()\n",
    "        if max_abs_value != 0:\n",
    "            exponent = int(np.floor(np.log10(max_abs_value)))\n",
    "            scale_notation = f'1e{exponent}'\n",
    "        else:\n",
    "            scale_notation = '1e0'\n",
    "\n",
    "        # Apply filters if after_preprocessing is True and if the column is 'runtime'\n",
    "        if after_preprocessing and column == 'runtime':\n",
    "            # Filter data to only show points outside the range [10, 90] as potential outliers\n",
    "            filtered_data = data_frame[(data_frame[column] < 10) | (data_frame[column] > 90)][column]\n",
    "            sns.boxplot(y=filtered_data, ax=axes[i], linewidth=linewidth, color='cyan')\n",
    "            axes[i].set_title(f'Boxplot of {column} (Filtered)\\nScale: {scale_notation}', fontsize=14)\n",
    "        else:\n",
    "            if column in exclude_zero_columns:\n",
    "                # Separate non-zero data and consider zeros as outliers\n",
    "                non_zero_data = data_frame[data_frame[column] != 0][column]\n",
    "                zero_outliers = data_frame[data_frame[column] == 0][column]\n",
    "\n",
    "                # Plot the box plot for non-zero data\n",
    "                sns.boxplot(y=non_zero_data, ax=axes[i], linewidth=linewidth, color='cyan')\n",
    "\n",
    "                # Plot zero values as outliers (by scattering them over the plot)\n",
    "                axes[i].scatter([0] * len(zero_outliers), zero_outliers, color='red', label='Zero Outliers', marker='x',\n",
    "                                zorder=5)\n",
    "\n",
    "                # Add a legend for outliers\n",
    "                axes[i].legend()\n",
    "            else:\n",
    "                # Use the original data for this column\n",
    "                sns.boxplot(data=data_frame, y=column, ax=axes[i], linewidth=linewidth, color='cyan')\n",
    "\n",
    "            # Add the title with the scale notation\n",
    "            axes[i].set_title(f'Boxplot of {column}\\nScale: {scale_notation}', fontsize=14)\n",
    "\n",
    "        axes[i].set_ylabel('Values', fontsize=12)\n",
    "        axes[i].grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "\n",
    "    # Hide unused subplots if any\n",
    "    for j in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust the layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "3fbacc93e6cb7d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_boxplots(df, numeric_features)",
   "id": "93e76a74de07e911",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part2 : Data Quality Assessment",
   "id": "a04debb5bea5cc26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def data_quality_assessment(df, excluded_columns):\n",
    "    # Filter columns to be analyzed\n",
    "    columns_to_analyze = [col for col in df.columns if col not in excluded_columns]\n",
    "\n",
    "    # Consistency Check\n",
    "    def check_consistency(data_frame, column):\n",
    "        if column == 'release_date':\n",
    "            invalid_dates = data_frame[pd.to_datetime(data_frame[column], errors='coerce').isna()]\n",
    "            return len(invalid_dates)\n",
    "        elif column == 'original_language':\n",
    "            invalid_languages = data_frame[~data_frame[column].str.match(r'^[a-z]{2}$', na=False)]\n",
    "            return len(invalid_languages)\n",
    "        elif column == 'status':\n",
    "            valid_statuses = ['Released', 'In Production', 'Canceled', 'Planned', 'Post Production', 'Rumored']\n",
    "            invalid_status = data_frame[~data_frame[column].isin(valid_statuses)]\n",
    "            return len(invalid_status)\n",
    "        elif column == 'adult':\n",
    "            valid_adult_values = ['True', 'False']\n",
    "            invalid_adult = data_frame[~data_frame[column].isin(valid_adult_values)]\n",
    "            return len(invalid_adult)\n",
    "        elif column in ['genres', 'production_companies', 'production_countries', 'spoken_languages']:\n",
    "            invalid_entries = data_frame[column][\n",
    "                data_frame[column].apply(lambda x: isinstance(x, str) and ',' not in x)]\n",
    "            return len(invalid_entries)\n",
    "        else:\n",
    "            return 'N/A'\n",
    "\n",
    "    # Currentness Check\n",
    "    def check_currentness(data_frame, column):\n",
    "        current_year = datetime.now().year\n",
    "        if column == 'release_date':\n",
    "            data_frame['Year'] = pd.to_datetime(data_frame[column], errors='coerce').dt.year\n",
    "            recent_data = data_frame[data_frame['Year'] >= current_year - 10]\n",
    "            return len(recent_data)\n",
    "        elif column == 'status':\n",
    "            mismatched_status = data_frame[\n",
    "                ((data_frame['status'] == 'Released') & (\n",
    "                            pd.to_datetime(data_frame['release_date'], errors='coerce') > datetime.now())) |\n",
    "                ((data_frame['status'] == 'In Production') & (\n",
    "                            pd.to_datetime(data_frame['release_date'], errors='coerce') <= datetime.now()))\n",
    "                ]\n",
    "            return len(mismatched_status)\n",
    "        elif column == 'popularity':\n",
    "            recent_popularity = data_frame[\n",
    "                (pd.to_datetime(data_frame['release_date'], errors='coerce').dt.year >= current_year - 5) & (\n",
    "                            data_frame[column] > 0)]\n",
    "            return len(recent_popularity)\n",
    "        else:\n",
    "            return 'N/A'\n",
    "\n",
    "    # Validity Check\n",
    "    def check_validity(data_frame, column):\n",
    "        if column == 'id':\n",
    "            unique_ids = data_frame['id'].nunique()\n",
    "            total_records = len(data_frame)\n",
    "            return total_records - unique_ids\n",
    "        elif column == 'vote_average':\n",
    "            invalid_values = data_frame[~data_frame[column].between(0, 10)]\n",
    "            return len(invalid_values)\n",
    "        elif column == 'vote_count':\n",
    "            invalid_values = data_frame[~data_frame[column].apply(lambda x: isinstance(x, int) and x >= 0)]\n",
    "            return len(invalid_values)\n",
    "        elif column in ['revenue', 'budget']:\n",
    "            invalid_values = data_frame[data_frame[column] < 0]\n",
    "            unreasonable_values = data_frame[data_frame[column] > 1e9]\n",
    "            return len(invalid_values) + len(unreasonable_values)\n",
    "        elif column == 'runtime':\n",
    "            invalid_values = data_frame[(data_frame[column] <= 0) | (data_frame[column] > 300)]\n",
    "            return len(invalid_values)\n",
    "        elif column == 'imdb_id':\n",
    "            invalid_imdb_ids = data_frame[~data_frame[column].str.match(r'^tt\\d{7,8}$', na=False)]\n",
    "            return len(invalid_imdb_ids)\n",
    "        elif column in ['genres', 'production_companies', 'production_countries', 'spoken_languages']:\n",
    "            invalid_entries = data_frame[column][\n",
    "                data_frame[column].apply(lambda x: isinstance(x, str) and ',' not in x)]\n",
    "            return len(invalid_entries)\n",
    "        elif column in ['overview', 'tagline']:\n",
    "            invalid_texts = data_frame[column][\n",
    "                data_frame[column].str.contains(r'[^\\w\\s,.!?\\'\\\"-]', regex=True, na=False)]\n",
    "            return len(invalid_texts)\n",
    "        else:\n",
    "            return 'N/A'\n",
    "\n",
    "    # Completeness Check\n",
    "    def check_completeness(data_frame, column):\n",
    "        essential_columns = [\n",
    "            'id', 'title', 'release_date', 'status', 'vote_count', 'vote_average',\n",
    "            'popularity', 'budget', 'revenue', 'imdb_id', 'overview', 'genres',\n",
    "            'production_companies', 'production_countries'\n",
    "        ]\n",
    "        if column in essential_columns:\n",
    "            null_count = data_frame[column].isnull().sum()\n",
    "            return null_count\n",
    "        else:\n",
    "            return 0  # For non-essential columns, assume completeness is not an issue\n",
    "\n",
    "    # Accuracy Check (now simplified to check data availability)\n",
    "    def check_accuracy(data_frame, column):\n",
    "        non_null_count = data_frame[column].notnull().sum()\n",
    "        accuracy = (non_null_count / len(data_frame)) * 100  # Percentage of non-null values\n",
    "        return f\"{accuracy:.2f}%\"\n",
    "\n",
    "    # Generate the table\n",
    "    data_quality_table = []\n",
    "\n",
    "    for column in columns_to_analyze:\n",
    "        record_count = len(df)\n",
    "        null_count = check_completeness(df, column)\n",
    "        consistency_issues = check_consistency(df, column)\n",
    "        currentness_count = check_currentness(df, column)\n",
    "        validity_issues = check_validity(df, column)\n",
    "        accuracy = check_accuracy(df, column)\n",
    "\n",
    "        data_quality_table.append({\n",
    "            'Feature Name': column,\n",
    "            'Number of Records': record_count,\n",
    "            'Number of Nulls': null_count,\n",
    "            'Accuracy (%)': accuracy,\n",
    "            'Completeness': null_count == 0,\n",
    "            'Validity Issues': validity_issues,\n",
    "            'Currentness Count': currentness_count,\n",
    "            'Consistency Issues': consistency_issues\n",
    "        })\n",
    "\n",
    "    # Create DataFrame for better visualization\n",
    "    quality_df = pd.DataFrame(data_quality_table)\n",
    "    return quality_df\n"
   ],
   "id": "4cde51c6352f367d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Call the function and pass the DataFrame and excluded columns\n",
    "excluded_columns = ['backdrop_path', 'homepage', 'imdb_id', 'poster_path', 'tagline']\n",
    "quality_df = data_quality_assessment(df, excluded_columns)\n",
    "\n",
    "# Display the result\n",
    "quality_df\n",
    "\n",
    "# Save the table as a CSV for reporting purposes (optional)\n",
    "# quality_df.to_csv('/mnt/data/data_quality_report.csv', index=False)"
   ],
   "id": "d4239c7d8c8ea898",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ad0b2ac-921f-4407-afeb-336a9ba93c5c",
   "metadata": {},
   "source": "# Part 3: Data Preparation"
  },
  {
   "cell_type": "code",
   "id": "f2d3d0c9-09f6-40ca-8c76-ca251c90226a",
   "metadata": {
    "scrolled": true
   },
   "source": "df.isnull().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Drop unused columns from the DataFrame.",
   "id": "821b1e3d01f2345b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unused_columns = ['backdrop_path', 'homepage', 'imdb_id', 'poster_path', 'tagline']\n",
    "df.drop(columns=unused_columns, axis=1, inplace=True)"
   ],
   "id": "7e716a1213e3a19a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Handling Missing, Inconsistent, and Erroneous Data\n",
    "\n",
    "* #### Replace Blank values"
   ],
   "id": "d60e572dadd36753"
  },
  {
   "cell_type": "code",
   "id": "d2274c3e-6132-4cf0-8e03-ada0cb420860",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Replace Blank values with the string 'null'\n",
    "df[['title', 'original_title', 'overview']] = df[['title', 'original_title', 'overview']].fillna('null')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* #### release_date ",
   "id": "34e7ae63ccdd7476"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_release_date_to_datetime(data_frame):\n",
    "    \"\"\"Convert 'release_date' column to datetime format.\"\"\"\n",
    "    data_frame['release_date'] = pd.to_datetime(data_frame['release_date'])\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def get_current_date():\n",
    "    \"\"\"Get the current date as a timestamp.\"\"\"\n",
    "    return pd.Timestamp.now()\n",
    "\n",
    "\n",
    "def mark_future_releases(data_frame, current_date):\n",
    "    \"\"\"Update records with 'release_date' after the current date.\"\"\"\n",
    "    future_df = data_frame[data_frame['release_date'] > current_date]\n",
    "\n",
    "    # Update the status, vote_average, and vote_count for future records\n",
    "    data_frame.loc[future_df.index, 'vote_count'] = 0\n",
    "    data_frame.loc[future_df.index, 'vote_average'] = 0\n",
    "    data_frame.loc[future_df.index, 'status'] = 'In Production'\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def remove_null_release_dates(data_frame):\n",
    "    \"\"\"Remove rows where 'release_date' is null.\"\"\"\n",
    "    data_frame.dropna(subset=['release_date'], inplace=True)\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def process_dataframe(data_frame):\n",
    "    \"\"\"Main function to execute all data processing steps.\"\"\"\n",
    "    data_frame = convert_release_date_to_datetime(data_frame)\n",
    "    current_date = get_current_date()\n",
    "    data_frame = mark_future_releases(data_frame, current_date)\n",
    "    data_frame = remove_null_release_dates(data_frame)\n",
    "    return data_frame"
   ],
   "id": "f212b95b4227e82f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Usage",
   "id": "ee0859091b59e943"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = process_dataframe(df)",
   "id": "6bbaae3ef8762d47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* #### popularity",
   "id": "d49f42deb8b60e07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace values greater than 100 with 100\n",
    "df['popularity'] = df['popularity'].clip(upper=100)"
   ],
   "id": "705c365c07a0b4bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* #### Prediction Model for Filling NaN in Language Field",
   "id": "9781bc620bb7892a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_data(data_frame):\n",
    "    \"\"\"Filter rows where 'spoken_languages' and 'production_countries' are not null.\"\"\"\n",
    "    return data_frame[data_frame['spoken_languages'].notna() & data_frame['production_countries'].notna()]\n",
    "\n",
    "\n",
    "def encode_features_and_target(data):\n",
    "    \"\"\"Encode 'production_countries' as features and 'spoken_languages' as target.\"\"\"\n",
    "    country_encoder = LabelEncoder()\n",
    "    language_encoder = LabelEncoder()\n",
    "\n",
    "    features_encoded = country_encoder.fit_transform(data['production_countries']).reshape(-1, 1)\n",
    "    target_encoded = language_encoder.fit_transform(data['spoken_languages'])\n",
    "\n",
    "    return features_encoded, target_encoded, country_encoder, language_encoder\n",
    "\n",
    "\n",
    "def train_model(features_train, target_train):\n",
    "    \"\"\"Train a Decision Tree Classifier and return the model.\"\"\"\n",
    "    decision_tree_model = DecisionTreeClassifier()\n",
    "    decision_tree_model.fit(features_train, target_train)\n",
    "    return decision_tree_model\n",
    "\n",
    "\n",
    "def predict_missing_languages(data_frame, Model, country_encoder, language_encoder):\n",
    "    \"\"\"Predict and fill missing 'spoken_languages' based on 'production_countries'.\"\"\"\n",
    "    rows_to_predict = data_frame[data_frame['spoken_languages'].isna() & data_frame['production_countries'].notna()]\n",
    "\n",
    "    features_to_predict = []\n",
    "    for country in rows_to_predict['production_countries']:\n",
    "        try:\n",
    "            features_to_predict.append(country_encoder.transform([country])[0])\n",
    "        except ValueError:\n",
    "            features_to_predict.append(-1)  # Fallback value for unseen labels\n",
    "\n",
    "    features_to_predict = np.array(features_to_predict).reshape(-1, 1)\n",
    "    predicted_encoded = Model.predict(features_to_predict)\n",
    "    predicted_languages = language_encoder.inverse_transform(predicted_encoded)\n",
    "\n",
    "    data_frame.loc[data_frame['spoken_languages'].isna() & data_frame[\n",
    "        'production_countries'].notna(), 'spoken_languages'] = predicted_languages\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def fill_null_columns(data_frame):\n",
    "    \"\"\"Fill 'production_companies', 'production_countries', 'spoken_languages' columns with 'null' value.\"\"\"\n",
    "    data_frame[['production_companies', 'production_countries', 'spoken_languages']] = data_frame[\n",
    "        ['production_companies', 'production_countries', 'spoken_languages']].fillna('null')\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def run_full_pipeline(data_frame):\n",
    "    \"\"\"Run the entire pipeline to prepare data, train the model, predict, and update the DataFrame.\"\"\"\n",
    "    # Step 1: Prepare data\n",
    "    filtered_data = prepare_data(data_frame)\n",
    "\n",
    "    # Step 2: Encode features and target\n",
    "    features_encoded, target_encoded, country_encoder, language_encoder = encode_features_and_target(filtered_data)\n",
    "\n",
    "    # Step 3: Split the data and train the model\n",
    "    features_train, features_test, target_train, target_test = train_test_split(features_encoded, target_encoded,\n",
    "                                                                                test_size=0.1, random_state=42)\n",
    "    classifier = train_model(features_train, target_train)\n",
    "\n",
    "    # Step 4: Evaluate model accuracy\n",
    "    predicted_test = classifier.predict(features_test)\n",
    "    print(f'Model Accuracy: {accuracy_score(target_test, predicted_test) * 100:.3f}%')\n",
    "\n",
    "    # Step 5: Predict missing values in the original DataFrame\n",
    "    data_frame = predict_missing_languages(data_frame, classifier, country_encoder, language_encoder)\n",
    "\n",
    "    # Step 6: Fill null values in specified columns\n",
    "    data_frame = fill_null_columns(data_frame)\n",
    "\n",
    "    return data_frame"
   ],
   "id": "5179b41d688acde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Usage",
   "id": "774428d15a77a874"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = run_full_pipeline(df)",
   "id": "8f0f96e266b1710c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "156fdfb1457a7d94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outlier Detection and Removal",
   "id": "37efc63101310db8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def replace_outliers(data_frame, Numeric_Features, Exclude_Zeros_Columns):\n",
    "    for feature in Numeric_Features:\n",
    "        # Ignore zeros if the column is in Exclude_Zeros_Columns\n",
    "        if feature in Exclude_Zeros_Columns:\n",
    "            feature_data = data_frame[feature][data_frame[feature] != 0]\n",
    "        else:\n",
    "            feature_data = data_frame[feature]\n",
    "\n",
    "        # Calculate IQR for outlier detection\n",
    "        Q1 = feature_data.quantile(0.25)\n",
    "        Q3 = feature_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Apply additional filtering for 'runtime' to ignore values between 10 and 90\n",
    "        if feature == 'runtime':\n",
    "            lower_bound = min(lower_bound, 10)\n",
    "            upper_bound = max(upper_bound, 90)\n",
    "\n",
    "        # Calculate the mean excluding zeros if necessary\n",
    "        if feature in Exclude_Zeros_Columns:\n",
    "            mean_value = feature_data.mean()\n",
    "        else:\n",
    "            mean_value = data_frame[feature].mean()\n",
    "\n",
    "        # Replace outliers with the calculated mean\n",
    "        data_frame[feature] = data_frame[feature].where(\n",
    "            (data_frame[feature] >= lower_bound) & (data_frame[feature] <= upper_bound),\n",
    "            mean_value\n",
    "        )\n",
    "\n",
    "    return data_frame"
   ],
   "id": "15ba53258f387a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Usage",
   "id": "8ee412049d0a4fd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_process = ['vote_average', 'vote_count', 'runtime', 'revenue', 'budget', 'popularity']\n",
    "exclude_zeros_columns = ['vote_count', 'revenue', 'budget', 'popularity']\n",
    "for column in columns_to_process:\n",
    "    df = replace_outliers(df, columns_to_process, exclude_zeros_columns)"
   ],
   "id": "fee1d710c3c4ec08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Text Processing\n",
    "* #### Stemming, Lemmatization, Stopwords Removal"
   ],
   "id": "52d157ab654f0748"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize stopwords, lemmatizer, and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Function to preprocess text and add a new column\n",
    "def preprocess_text_column(data_frame, column_name):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = data_frame.copy()\n",
    "\n",
    "    # Define the preprocessing function for each text entry\n",
    "    def preprocess_text(text):\n",
    "        # Check if the text is a string; if not, return an empty string\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "\n",
    "        # Remove stopwords and apply lemmatization and stemming\n",
    "        processed_words = []\n",
    "        for word in tokens:\n",
    "            if word not in stop_words:\n",
    "                lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                stemmed_word = stemmer.stem(lemmatized_word)\n",
    "                processed_words.append(stemmed_word)\n",
    "\n",
    "        # Join processed words back into a single string\n",
    "        return ' '.join(processed_words)\n",
    "\n",
    "    # Process the column and add the results as a new column\n",
    "    processed_column_name = f\"processed_{column_name}\"\n",
    "    df_copy[processed_column_name] = df_copy[column_name].apply(preprocess_text)\n",
    "\n",
    "    return df_copy"
   ],
   "id": "6638034d787fb87e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Usage",
   "id": "19c60bc6c5fa8158"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = preprocess_text_column(df, 'overview')\n",
    "df[['overview', 'processed_overview']]"
   ],
   "id": "7e5c5c888a36a786",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encoding Categorical Data",
   "id": "14f5265c2a49cc38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_text_to_numeric(data_frame):\n",
    "    # 1. Convert 'original_language' to One-Hot Encoding\n",
    "    data_frame = pd.get_dummies(data_frame, columns=['original_language'], prefix='lang')\n",
    "    \n",
    "    # 2. Convert 'genres' to Multi-Label Binary Encoding\n",
    "    # Split genres by comma and apply One-Hot Encoding\n",
    "    genres_split = data_frame['genres'].str.get_dummies(sep=',')\n",
    "    data_frame = pd.concat([data_frame, genres_split], axis=1)\n",
    "    \n",
    "    # 3. Convert 'production_companies' to Multi-Label Binary Encoding\n",
    "    # Split production companies by comma and apply One-Hot Encoding\n",
    "    companies_split = data_frame['production_companies'].str.get_dummies(sep=',')\n",
    "    data_frame = pd.concat([data_frame, companies_split], axis=1)\n",
    "    \n",
    "    # 4. Convert 'production_countries' to Multi-Label Binary Encoding\n",
    "    # Split production countries by comma and apply One-Hot Encoding\n",
    "    countries_split = data_frame['production_countries'].str.get_dummies(sep=',')\n",
    "    data_frame = pd.concat([data_frame, countries_split], axis=1)\n",
    "    \n",
    "    # 5. Convert 'spoken_languages' to Multi-Label Binary Encoding\n",
    "    # Split spoken languages by comma and apply One-Hot Encoding\n",
    "    languages_split = data_frame['spoken_languages'].str.get_dummies(sep=',')\n",
    "    data_frame = pd.concat([data_frame, languages_split], axis=1)\n",
    "    \n",
    "    # 6. Convert 'status' to Label Encoding\n",
    "    # Map each status to an integer\n",
    "    status_mapping = {'Released': 0, 'In Production': 1, 'Planned': 2, 'Post Production': 3, 'Rumored': 4, 'Canceled': 5}\n",
    "    data_frame['status'] = data_frame['status'].map(status_mapping)\n",
    "    \n",
    "    # 7. Convert 'adult' to binary (0 and 1)\n",
    "    data_frame['adult'] = data_frame['adult'].map({'False': 0, 'True': 1})\n",
    "    \n",
    "    return data_frame\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'df' is your DataFrame with the necessary columns\n",
    "df = convert_text_to_numeric(df)\n",
    "\n",
    "# Display the first few rows to verify results\n",
    "df.head(20)"
   ],
   "id": "786d5a1df3d556c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Converting Numerical Data to Categorical (Discretization)",
   "id": "d38fc020be6e3d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_to_categorical(data_frame):\n",
    "    # 1. Convert 'budget' to categorical\n",
    "    data_frame['budget_category'] = pd.cut(data_frame['budget'],\n",
    "                                           bins=[0, 5e7, 1e8, float('inf')],\n",
    "                                           labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "    # 2. Convert 'revenue' to categorical\n",
    "    data_frame['revenue_category'] = pd.cut(data_frame['revenue'],\n",
    "                                            bins=[0, 5e7, 1.5e8, 5e8, float('inf')],\n",
    "                                            labels=['Flop', 'Average', 'Hit', 'Blockbuster'])\n",
    "\n",
    "    # 3. Convert 'popularity' to categorical\n",
    "    data_frame['popularity_category'] = pd.cut(data_frame['popularity'],\n",
    "                                               bins=[-float('inf'), 20, 40, 60, float('inf')],\n",
    "                                               labels=['Low', 'Moderate', 'High', 'Very High'])\n",
    "\n",
    "    # 4. Convert 'runtime' to categorical\n",
    "    data_frame['runtime_category'] = pd.cut(data_frame['runtime'],\n",
    "                                            bins=[0, 40, 90, 120, float('inf')],\n",
    "                                            labels=['Short', 'Standard', 'Long', 'Very Long'])\n",
    "\n",
    "    # 5. Convert 'vote_average' to categorical\n",
    "    data_frame['vote_average_category'] = pd.cut(data_frame['vote_average'],\n",
    "                                                 bins=[0, 4, 6, 7.5, 10],\n",
    "                                                 labels=['Poor', 'Average', 'Good', 'Excellent'])\n",
    "\n",
    "    # 6. Convert 'release_date' to categorical by decade\n",
    "    # Extract the year from 'release_date' first\n",
    "    data_frame['year'] = pd.to_datetime(data_frame['release_date'], errors='coerce').dt.year\n",
    "    # Define categories based on decades\n",
    "    data_frame['release_decade'] = pd.cut(data_frame['year'],\n",
    "                                          bins=[1900, 2000, 2010, 2020, float('inf')],\n",
    "                                          labels=['Before 2000', '2000s', '2010s', '2020s'])\n",
    "\n",
    "    return data_frame"
   ],
   "id": "e07f8011176d48f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Usage",
   "id": "d017bfae91d5f7e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = convert_to_categorical(df)\n",
    "\n",
    "# Display the first few rows to verify results\n",
    "df[['budget_category', 'revenue_category', 'popularity_category', 'runtime_category',\n",
    "    'vote_average_category', 'release_decade']].head()"
   ],
   "id": "c0e285b1b75c45b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalization of numerical data",
   "id": "53645a56aa4c78b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_features(df):\n",
    "    # Initialize scalers\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    standard_scaler = StandardScaler()\n",
    "\n",
    "    # 1. Normalize 'popularity' with Min-Max scaling (0 to 1)\n",
    "    df['popularity_normalized'] = min_max_scaler.fit_transform(df[['popularity']])\n",
    "\n",
    "    # 2. Normalize 'runtime' with Standard scaling (Z-score)\n",
    "    df['runtime_normalized'] = standard_scaler.fit_transform(df[['runtime']])\n",
    "\n",
    "    # 3. Normalize 'vote_average' with Standard scaling (Z-score)\n",
    "    df['vote_average_normalized'] = standard_scaler.fit_transform(df[['vote_average']])\n",
    "\n",
    "    # 4. Log transformation and Min-Max scaling for 'budget' to reduce skewness\n",
    "    df['budget_log'] = np.log1p(df['budget'])  # log(1 + x) to handle zero values\n",
    "    df['budget_normalized'] = min_max_scaler.fit_transform(df[['budget_log']])\n",
    "\n",
    "    # 5. Log transformation and Min-Max scaling for 'revenue' to reduce skewness\n",
    "    df['revenue_log'] = np.log1p(df['revenue'])  # log(1 + x) to handle zero values\n",
    "    df['revenue_normalized'] = min_max_scaler.fit_transform(df[['revenue_log']])\n",
    "\n",
    "    # 6. Normalize 'vote_count' with Min-Max scaling (0 to 1)\n",
    "    df['vote_count_normalized'] = min_max_scaler.fit_transform(df[['vote_count']])\n",
    "\n",
    "    # 7. Min-Max scaling for 'Year' for clustering based on release date\n",
    "    df['Year_normalized'] = min_max_scaler.fit_transform(df[['Year']])\n",
    "\n",
    "    # Drop intermediate log-transformed columns to keep the dataset clean\n",
    "    df.drop(columns=['budget_log', 'revenue_log'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = normalize_features(df)"
   ],
   "id": "5963e9b8e22c9b48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate Net Gain ",
   "id": "a189dbd813373e0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['net_gain'] = df['revenue'] - df['budget']",
   "id": "eb9ce6c036965938",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a901ca0a-8aa3-4bf3-9dcf-dda5ec9588b8",
   "metadata": {},
   "source": "df.isnull().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0b8561c-8271-443b-91b7-80a0010237f8",
   "metadata": {},
   "source": [
    "# Part 1 Data Recognition\n",
    "### Creating a Data Recognition Table"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_summary = summarize_data(df, columns_to_process)\n",
    "\n",
    "final_summary"
   ],
   "id": "7040aed995454d96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating a box plot for numerical data",
   "id": "33ddf31a61aff5aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_boxplots(df, columns_to_process, exclude_zeros_columns, after_preprocessing=True)",
   "id": "b2e5619a750ebbce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c066a70a-43d9-4be6-aaf7-27c7fb1b2986",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "9d4a6d2c-c388-4bbf-84ed-fd717c48c775",
   "metadata": {},
   "source": [
    "## Part1.3 (association inspection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c5ca3-4aa7-4a56-a06e-b1d56d11a678",
   "metadata": {},
   "source": [
    "###  Budget and Revenue Scatter plot and association inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "fce60555-4cfa-4568-90b3-89e7ed07a892",
   "metadata": {},
   "source": [
    "# Assuming your DataFrame 'data_frame' contains 'budget' and 'revenue' columns\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['budget'], df['revenue'], color='blue', alpha=0.5)\n",
    "plt.title('Relationship between Revenue and Budget')\n",
    "plt.xlabel('Budget')\n",
    "plt.ylabel('Revenue')\n",
    "plt.grid(True)\n",
    "\n",
    "# Fit a linear regression model\n",
    "X = df['budget'].values.reshape(-1, 1)  # Reshape for sklearn\n",
    "y = df['revenue'].values\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot the best-fit line\n",
    "plt.plot(df['budget'], y_pred, color='red', linewidth=2, label='Best-fit line')\n",
    "\n",
    "# Show plot with best-fit line\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Pearson association_score coefficient\n",
    "association_score = df['budget'].corr(df['revenue'])\n",
    "print(f'Correlation between Budget and Revenue: {association_score:.2f}')\n",
    "\n",
    "# Calculate R² score (accuracy of the line)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f'R² score (accuracy of the line): {r2:.2f}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20784805-3d77-40c1-b503-24d7475d6aa0",
   "metadata": {},
   "source": [
    "### vote_average and vote_count  histogram inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "90fced8e-d6ce-4f71-9bcf-fde68a0b23c7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "\n",
    "# Assuming your DataFrame 'data_frame' contains 'vote_average' and 'vote_count' columns\n",
    "\n",
    "# Create bins for vote_average\n",
    "bins = 10\n",
    "df['vote_average_binned'] = pd.cut(df['vote_average'], bins=bins)\n",
    "\n",
    "# Group by the binned vote_average and sum the vote_count\n",
    "binned_data = df.groupby('vote_average_binned')['vote_count'].sum().reset_index()\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(binned_data['vote_average_binned'].astype(str), binned_data['vote_count'], color='blue', alpha=0.7)\n",
    "plt.title('Vote Count by Vote Average')\n",
    "plt.xlabel('Vote Average')\n",
    "plt.ylabel('Vote Count')\n",
    "plt.xticks(rotation=45)  # Rotate x labels for better visibility\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "500da882-ceae-4b0d-8b11-5c9fdc432d5f",
   "metadata": {},
   "source": [
    "### Release date Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e8acc93-1859-44be-bf68-fec1ddb882ac",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Sample DataFrame\n",
    "# data_frame = pd.read_csv('your_data.csv')  # Load your DataFrame\n",
    "\n",
    "# Step 1: Convert release_date to datetime format\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "\n",
    "# Step 2: Extract the year from the release_date\n",
    "df['year'] = df['release_date'].dt.year\n",
    "\n",
    "# Step 3: Filter the DataFrame based on the desired status (replace 'desired_status' with your status value)\n",
    "desired_status = 'Released'  # Replace with the status you are interested in\n",
    "filtered_data_frame = df[df['status'] == desired_status]\n",
    "\n",
    "# Step 4: Group by year and count the records\n",
    "yearly_counts = filtered_data_frame['year'].value_counts().sort_index()\n",
    "\n",
    "# Step 5: Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(yearly_counts.index, yearly_counts.values, marker='o')\n",
    "plt.title(f\"Number of Records Released Each Year with Status '{desired_status}'\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.grid()\n",
    "plt.xticks(yearly_counts.index)  # Show all years on the x-axis\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4efd3d41-d38b-4607-8412-173212600203",
   "metadata": {},
   "source": [
    "### Genres inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "0be2f4ae-730f-423b-b5dd-68bb303bfc83",
   "metadata": {},
   "source": [
    "# Step 1: Split the 'genres' column into separate rows\n",
    "data_frame_genres = df.assign(genres=df['genres'].str.split(',')).explode('genres')\n",
    "\n",
    "# Step 2: Remove any extra spaces around genres\n",
    "data_frame_genres['genres'] = data_frame_genres['genres'].str.strip()\n",
    "\n",
    "# Step 3: Group by genres and calculate the mean vote_average and total revenue for each genre\n",
    "genre_stats = data_frame_genres.groupby('genres').agg(\n",
    "    total_movies=('genres', 'size'),\n",
    "    average_vote=('vote_average', 'mean'),\n",
    "    total_revenue=('revenue', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Step 4: Sort by total_movies or total_revenue to find the most popular genres\n",
    "genre_stats_sorted = genre_stats.sort_values(by=['total_movies', 'total_revenue'], ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(genre_stats_sorted)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32686f00-3dd5-491f-a627-bdd6d68edbc8",
   "metadata": {},
   "source": [
    "### production_companies inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ae347f6-bcd1-4475-aa1a-3de45417950b",
   "metadata": {},
   "source": [
    "# Step 1: Split 'production_companies' column into separate rows\n",
    "data_frame_companies = df.assign(production_companies=df['production_companies'].str.split(',')).explode(\n",
    "    'production_companies')\n",
    "\n",
    "# Step 2: Remove any extra spaces around company names\n",
    "data_frame_companies['production_companies'] = data_frame_companies['production_companies'].str.strip()\n",
    "\n",
    "# Step 3: Filter movies with high ratings (e.g., vote_average >= 7.0)\n",
    "high_rated_movies = data_frame_companies[data_frame_companies['vote_average'] >= 7.0]\n",
    "\n",
    "# Step 4: Group by production companies and calculate metrics\n",
    "company_stats = high_rated_movies.groupby('production_companies').agg(\n",
    "    num_high_rated_movies=('production_companies', 'size'),  # Number of high-rated movies\n",
    "    avg_popularity=('popularity', 'mean'),  # Average popularity of movies\n",
    "    avg_revenue=('revenue', 'mean'),  # Average revenue of movies\n",
    "    avg_vote=('vote_average', 'mean')  # Average vote rating of movies\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Sort by number of high-rated movies or revenue (optional)\n",
    "company_stats_sorted = company_stats.sort_values(by=['num_high_rated_movies', 'avg_revenue'], ascending=False)\n",
    "\n",
    "# Display the result\n",
    "pd.DataFrame(company_stats_sorted).head(7)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "696df84b-0352-47aa-a4c5-966043c20f29",
   "metadata": {},
   "source": [
    "### adult inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "afffd886-2ebe-4647-8f59-b133077213bd",
   "metadata": {},
   "source": [
    "# Step 1: Separate adult and non-adult movies\n",
    "adult_movies = df[df['adult'] == True]\n",
    "non_adult_movies = df[df['adult'] == False]\n",
    "\n",
    "# Step 2: Calculate metrics for adult movies\n",
    "adult_stats = adult_movies.agg({\n",
    "    'revenue': 'mean',\n",
    "    'vote_average': 'mean',\n",
    "    'popularity': 'mean'\n",
    "})\n",
    "\n",
    "# Step 3: Calculate metrics for non-adult movies\n",
    "non_adult_stats = non_adult_movies.agg({\n",
    "    'revenue': 'mean',\n",
    "    'vote_average': 'mean',\n",
    "    'popularity': 'mean'\n",
    "})\n",
    "\n",
    "# Step 4: Combine the results for comparison\n",
    "comparison_stats = pd.DataFrame({\n",
    "    'Metric': ['Revenue', 'Vote Average', 'Popularity'],\n",
    "    'Adult': [adult_stats['revenue'], adult_stats['vote_average'], adult_stats['popularity']],\n",
    "    'Non-Adult': [non_adult_stats['revenue'], non_adult_stats['vote_average'], non_adult_stats['popularity']]\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "pd.DataFrame(comparison_stats)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f88b838-8b87-4136-85c5-2673d376bdcb",
   "metadata": {},
   "source": [
    "### budget/revenue inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "91aaedcc-45f3-4b3c-9aa3-93f48feffed6",
   "metadata": {},
   "source": [
    "# Remove rows where the budget is zero or where revenue or budget data is missing (NaN values)\n",
    "data_frame_clean = df.dropna(subset=['revenue', 'budget'])\n",
    "\n",
    "# budget can't be 0\n",
    "data_frame_clean = data_frame_clean[data_frame_clean['budget'] > 0]\n",
    "\n",
    "# Create a new column for the revenue-to-budget ratio\n",
    "data_frame_clean['revenue_to_budget'] = data_frame_clean['revenue'] / data_frame_clean['budget']\n",
    "\n",
    "# Sort the movies based on the revenue-to-budget ratio in descending order\n",
    "data_frame_sorted = data_frame_clean.sort_values(by='revenue_to_budget', ascending=False)\n",
    "\n",
    "# Display the top 10 movies in terms of financial success\n",
    "top_10_successful_movies = data_frame_sorted[['title', 'revenue', 'budget', 'revenue_to_budget']].head(10)\n",
    "\n",
    "pd.DataFrame(top_10_successful_movies)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58049d17-5631-40c3-8e19-5e2edaebee6e",
   "metadata": {},
   "source": "### spoken languages analysis"
  },
  {
   "cell_type": "code",
   "id": "c5fc5ecb-e132-4d03-8e08-41e27cc1497c",
   "metadata": {},
   "source": [
    "# Step 1: Clean and split the 'spoken_languages' column\n",
    "# Remove quotes and split the languages by comma\n",
    "df['spoken_languages'] = df['spoken_languages'].str.replace(\"'\", \"\").str.split(', ')\n",
    "\n",
    "# Step 2: Explode the list to separate rows for each language\n",
    "data_frame_exploded = df.explode('spoken_languages')\n",
    "\n",
    "# Step 3: Count the occurrences of each language\n",
    "language_counts = data_frame_exploded['spoken_languages'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57bd17b6-3a0f-4619-8af0-2890d8747780",
   "metadata": {},
   "source": [
    "# Display the results\n",
    "pd.DataFrame(language_counts).head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63320375-52b6-488c-a285-5d30634ebe67",
   "metadata": {},
   "source": [
    "### popularity vs vote_average inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c65de49-9eee-442c-97da-674ec04e2825",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 1: Calculate the association_score between 'runtime' and 'budget'\n",
    "association_score = df['vote_average'].corr(df['popularity'])\n",
    "print(f'Correlation between runtime and budget: {association_score}')\n",
    "\n",
    "# Step 2: Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['vote_average'], df['popularity'], color='blue', alpha=0.7)\n",
    "plt.title('Scatter Plot of vote_average vs popularity')\n",
    "plt.xlabel('vote_average ')\n",
    "plt.ylabel('popularity')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f88540b0-3150-4840-bb7d-84e0c89d964c",
   "metadata": {},
   "source": [
    "###  vote_count vs vote_average inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "94c969f0-b3fa-430a-86b5-2ae94b35f080",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 1: Calculate the association_score between 'runtime' and 'budget'\n",
    "association_score = df['vote_count'].corr(df['vote_average'])\n",
    "print(f'Correlation between runtime and budget: {association_score}')\n",
    "\n",
    "# Step 2: Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['vote_count'], df['vote_average'], color='blue', alpha=0.7)\n",
    "plt.title('Scatter Plot of vote_count vs vote_average')\n",
    "plt.xlabel('vote_count ')\n",
    "plt.ylabel('vote_average')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf08a0df-a763-499f-9f71-6b4816501d1a",
   "metadata": {},
   "source": [
    "### runtime and budget association inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "c12fec85-d132-4ef3-b5d1-3e3f987eb7c9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step 1: Calculate the association_score between 'runtime' and 'budget'\n",
    "association_score = df['runtime'].corr(df['budget'])\n",
    "print(f'Correlation between runtime and budget: {association_score}')\n",
    "\n",
    "# Step 2: Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['runtime'], df['budget'], color='blue', alpha=0.7)\n",
    "plt.title('Scatter Plot of Runtime vs Budget')\n",
    "plt.xlabel('Runtime (minutes)')\n",
    "plt.ylabel('Budget (currency units)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "274b3819-e2ad-4993-8b64-4c017b93dde8",
   "metadata": {},
   "source": [
    "## Filter the records that are after 2014  (last 10 years ago)"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ba8d1b4-021e-4fdc-8df1-ac6ad363dafd",
   "metadata": {},
   "source": [
    "# Step 1: Convert 'release_date' to datetime format\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "\n",
    "# Step 2: Filter the DataFrame to drop rows with 'release_date' less than 2014-01-01\n",
    "data_frame_filtered = df[df['release_date'] >= '2014-01-01']\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data_frame_filtered.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "650b68c9-c055-468b-873f-28b273a89199",
   "metadata": {},
   "source": [
    "### Check how many  records that have budget=0 or null and revenue != 0 or not null"
   ]
  },
  {
   "cell_type": "code",
   "id": "e492fc83-5bc4-46ce-8d28-053163894cb1",
   "metadata": {},
   "source": [
    "# Condition to check budget = 0 or null and revenue != 0 or not null (consistency)\n",
    "condition = (df['budget'].isnull() | (df['budget'] == 0)) & (df['revenue'].notnull() & (df['revenue'] != 0))\n",
    "\n",
    "# Count the number of records that satisfy the condition\n",
    "count = df[condition].shape[0]\n",
    "print(count)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
